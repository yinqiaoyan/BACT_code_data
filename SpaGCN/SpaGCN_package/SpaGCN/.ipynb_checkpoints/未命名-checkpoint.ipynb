{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93beb179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,csv,re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import math\n",
    "from scipy.sparse import issparse\n",
    "import random, torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.colors as clr\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04ef7bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers.py\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3d34d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from sklearn.cluster import KMeans\n",
    "import torch.optim as optim\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "# from . layers import GraphConvolution\n",
    "\n",
    "\n",
    "class simple_GC_DEC(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, alpha=0.2):\n",
    "        super(simple_GC_DEC, self).__init__()\n",
    "        self.gc = GraphConvolution(nfeat, nhid)\n",
    "        self.nhid=nhid\n",
    "        #self.mu determined by the init method\n",
    "        self.alpha=alpha\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x=self.gc(x, adj)\n",
    "        q = 1.0 / ((1.0 + torch.sum((x.unsqueeze(1) - self.mu)**2, dim=2) / self.alpha) + 1e-8)\n",
    "        q = q**(self.alpha+1.0)/2.0\n",
    "        q = q / torch.sum(q, dim=1, keepdim=True)\n",
    "        return x, q\n",
    "\n",
    "    def loss_function(self, p, q):\n",
    "        def kld(target, pred):\n",
    "            return torch.mean(torch.sum(target*torch.log(target/(pred+1e-6)), dim=1))\n",
    "        loss = kld(p, q)\n",
    "        return loss\n",
    "\n",
    "    def target_distribution(self, q):\n",
    "        #weight = q ** 2 / q.sum(0)\n",
    "        #return torch.transpose((torch.transpose(weight,0,1) / weight.sum(1)),0,1)e\n",
    "        p = q**2 / torch.sum(q, dim=0)\n",
    "        p = p / torch.sum(p, dim=1, keepdim=True)\n",
    "        return p\n",
    "\n",
    "    def fit(self, X,adj,  lr=0.001, max_epochs=5000, update_interval=3, trajectory_interval=50,weight_decay=5e-4,opt=\"sgd\",init=\"louvain\",n_neighbors=10,res=0.4,n_clusters=10,init_spa=True,tol=1e-3):\n",
    "        self.trajectory=[]\n",
    "        if opt==\"sgd\":\n",
    "            optimizer = optim.SGD(self.parameters(), lr=lr, momentum=0.9)\n",
    "        elif opt==\"admin\":\n",
    "            optimizer = optim.Adam(self.parameters(),lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        features= self.gc(torch.FloatTensor(X),torch.FloatTensor(adj))\n",
    "        #----------------------------------------------------------------        \n",
    "        if init==\"kmeans\":\n",
    "            print(\"Initializing cluster centers with kmeans, n_clusters known\")\n",
    "            self.n_clusters=n_clusters\n",
    "            kmeans = KMeans(self.n_clusters, n_init=20)\n",
    "            if init_spa:\n",
    "                #------Kmeans use exp and spatial\n",
    "                y_pred = kmeans.fit_predict(features.detach().numpy())\n",
    "            else:\n",
    "                #------Kmeans only use exp info, no spatial\n",
    "                y_pred = kmeans.fit_predict(X)  #Here we use X as numpy\n",
    "        elif init==\"louvain\":\n",
    "            print(\"Initializing cluster centers with louvain, resolution = \", res)\n",
    "            if init_spa:\n",
    "                adata=sc.AnnData(features.detach().numpy())\n",
    "            else:\n",
    "                adata=sc.AnnData(X)\n",
    "            sc.pp.neighbors(adata, n_neighbors=n_neighbors)\n",
    "            sc.tl.louvain(adata,resolution=res)\n",
    "            y_pred=adata.obs['louvain'].astype(int).to_numpy()\n",
    "            self.n_clusters=len(np.unique(y_pred))\n",
    "        #----------------------------------------------------------------\n",
    "        y_pred_last = y_pred\n",
    "        self.mu = Parameter(torch.Tensor(self.n_clusters, self.nhid))\n",
    "        X=torch.FloatTensor(X)\n",
    "        adj=torch.FloatTensor(adj)\n",
    "        self.trajectory.append(y_pred)\n",
    "        features=pd.DataFrame(features.detach().numpy(),index=np.arange(0,features.shape[0]))\n",
    "        Group=pd.Series(y_pred,index=np.arange(0,features.shape[0]),name=\"Group\")\n",
    "        Mergefeature=pd.concat([features,Group],axis=1)\n",
    "        cluster_centers=np.asarray(Mergefeature.groupby(\"Group\").mean())\n",
    "        \n",
    "        self.mu.data.copy_(torch.Tensor(cluster_centers))\n",
    "        self.train()\n",
    "        for epoch in range(max_epochs):\n",
    "            if epoch%update_interval == 0:\n",
    "                _, q = self.forward(X,adj)\n",
    "                p = self.target_distribution(q).data\n",
    "            if epoch%10==0:\n",
    "                print(\"Epoch \", epoch) \n",
    "            optimizer.zero_grad()\n",
    "            z,q = self(X, adj)\n",
    "            loss = self.loss_function(p, q)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch%trajectory_interval == 0:\n",
    "                self.trajectory.append(torch.argmax(q, dim=1).data.cpu().numpy())\n",
    "\n",
    "            #Check stop criterion\n",
    "            y_pred = torch.argmax(q, dim=1).data.cpu().numpy()\n",
    "            delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / X.shape[0]\n",
    "            y_pred_last = y_pred\n",
    "            if epoch>0 and (epoch-1)%update_interval == 0 and delta_label < tol:\n",
    "                print('delta_label ', delta_label, '< tol ', tol)\n",
    "                print(\"Reach tolerance threshold. Stopping training.\")\n",
    "                print(\"Total epoch:\", epoch)\n",
    "                break\n",
    "\n",
    "\n",
    "    def fit_with_init(self, X,adj, init_y, lr=0.001, max_epochs=5000, update_interval=1, weight_decay=5e-4,opt=\"sgd\"):\n",
    "        print(\"Initializing cluster centers with kmeans.\")\n",
    "        if opt==\"sgd\":\n",
    "            optimizer = optim.SGD(self.parameters(), lr=lr, momentum=0.9)\n",
    "        elif opt==\"admin\":\n",
    "            optimizer = optim.Adam(self.parameters(),lr=lr, weight_decay=weight_decay)\n",
    "        X=torch.FloatTensor(X)\n",
    "        adj=torch.FloatTensor(adj)\n",
    "        features, _ = self.forward(X,adj)\n",
    "        features=pd.DataFrame(features.detach().numpy(),index=np.arange(0,features.shape[0]))\n",
    "        Group=pd.Series(init_y,index=np.arange(0,features.shape[0]),name=\"Group\")\n",
    "        Mergefeature=pd.concat([features,Group],axis=1)\n",
    "        cluster_centers=np.asarray(Mergefeature.groupby(\"Group\").mean())\n",
    "        self.mu.data.copy_(torch.Tensor(cluster_centers))\n",
    "        self.train()\n",
    "        for epoch in range(max_epochs):\n",
    "            if epoch%update_interval == 0:\n",
    "                _, q = self.forward(torch.FloatTensor(X),torch.FloatTensor(adj))\n",
    "                p = self.target_distribution(q).data\n",
    "            X=torch.FloatTensor(X)\n",
    "            adj=torch.FloatTensor(adj)\n",
    "            optimizer.zero_grad()\n",
    "            z,q = self(X, adj)\n",
    "            loss = self.loss_function(p, q)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def predict(self, X, adj):\n",
    "        z,q = self(torch.FloatTensor(X),torch.FloatTensor(adj))\n",
    "        return z, q\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GC_DEC(nn.Module):\n",
    "    def __init__(self, nfeat, nhid1,nhid2, n_clusters=None, dropout=0.5,alpha=0.2):\n",
    "        super(GC_DEC, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid1)\n",
    "        self.gc2 = GraphConvolution(nhid1, nhid2)\n",
    "        self.dropout = dropout\n",
    "        self.mu = Parameter(torch.Tensor(n_clusters, nhid2))\n",
    "        self.n_clusters=n_clusters\n",
    "        self.alpha=alpha\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x=self.gc1(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, self.dropout, training=True)\n",
    "        x = self.gc2(x, adj)\n",
    "        q = 1.0 / ((1.0 + torch.sum((x.unsqueeze(1) - self.mu)**2, dim=2) / self.alpha) + 1e-6)\n",
    "        q = q**(self.alpha+1.0)/2.0\n",
    "        q = q / torch.sum(q, dim=1, keepdim=True)\n",
    "        return x, q\n",
    "\n",
    "    def loss_function(self, p, q):\n",
    "        def kld(target, pred):\n",
    "            return torch.mean(torch.sum(target*torch.log(target/(pred+1e-6)), dim=1))\n",
    "        loss = kld(p, q)\n",
    "        return loss\n",
    "\n",
    "    def target_distribution(self, q):\n",
    "        #weight = q ** 2 / q.sum(0)\n",
    "        #return torch.transpose((torch.transpose(weight,0,1) / weight.sum(1)),0,1)e\n",
    "        p = q**2 / torch.sum(q, dim=0)\n",
    "        p = p / torch.sum(p, dim=1, keepdim=True)\n",
    "        return p\n",
    "\n",
    "    def fit(self, X,adj, lr=0.001, max_epochs=10, update_interval=5, weight_decay=5e-4,opt=\"sgd\",init=\"louvain\",n_neighbors=10,res=0.4):\n",
    "        self.trajectory=[]\n",
    "        print(\"Initializing cluster centers with kmeans.\")\n",
    "        if opt==\"sgd\":\n",
    "            optimizer = optim.SGD(self.parameters(), lr=lr, momentum=0.9)\n",
    "        elif opt==\"admin\":\n",
    "            optimizer = optim.Adam(self.parameters(),lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        features, _ = self.forward(torch.FloatTensor(X),torch.FloatTensor(adj))\n",
    "        #----------------------------------------------------------------\n",
    "        \n",
    "        if init==\"kmeans\":\n",
    "            #Kmeans only use exp info, no spatial\n",
    "            #kmeans = KMeans(self.n_clusters, n_init=20)\n",
    "            #y_pred = kmeans.fit_predict(X)  #Here we use X as numpy\n",
    "            #Kmeans use exp and spatial\n",
    "            kmeans = KMeans(self.n_clusters, n_init=20)\n",
    "            y_pred = kmeans.fit_predict(features.detach().numpy())\n",
    "        elif init==\"louvain\":\n",
    "            adata=sc.AnnData(features.detach().numpy())\n",
    "            sc.pp.neighbors(adata, n_neighbors=n_neighbors)\n",
    "            sc.tl.louvain(adata,resolution=res)\n",
    "            y_pred=adata.obs['louvain'].astype(int).to_numpy()\n",
    "        #----------------------------------------------------------------\n",
    "        X=torch.FloatTensor(X)\n",
    "        adj=torch.FloatTensor(adj)\n",
    "        self.trajectory.append(y_pred)\n",
    "        features=pd.DataFrame(features.detach().numpy(),index=np.arange(0,features.shape[0]))\n",
    "        Group=pd.Series(y_pred,index=np.arange(0,features.shape[0]),name=\"Group\")\n",
    "        Mergefeature=pd.concat([features,Group],axis=1)\n",
    "        cluster_centers=np.asarray(Mergefeature.groupby(\"Group\").mean())\n",
    "        \n",
    "        self.mu.data.copy_(torch.Tensor(cluster_centers))\n",
    "        self.train()\n",
    "        for epoch in range(max_epochs):\n",
    "            if epoch%update_interval == 0:\n",
    "                _, q = self.forward(X,adj)\n",
    "                p = self.target_distribution(q).data\n",
    "            if epoch%100==0:\n",
    "                print(\"Epoch \", epoch) \n",
    "            optimizer.zero_grad()\n",
    "            z,q = self(X, adj)\n",
    "            loss = self.loss_function(p, q)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            self.trajectory.append(torch.argmax(q, dim=1).data.cpu().numpy())\n",
    "\n",
    "    def fit_with_init(self, X,adj, init_y, lr=0.001, max_epochs=10, update_interval=1, weight_decay=5e-4,opt=\"sgd\"):\n",
    "        print(\"Initializing cluster centers with kmeans.\")\n",
    "        if opt==\"sgd\":\n",
    "            optimizer = optim.SGD(self.parameters(), lr=lr, momentum=0.9)\n",
    "        elif opt==\"admin\":\n",
    "            optimizer = optim.Adam(self.parameters(),lr=lr, weight_decay=weight_decay)\n",
    "        X=torch.FloatTensor(X)\n",
    "        adj=torch.FloatTensor(adj)\n",
    "        features, _ = self.forward(X,adj)\n",
    "        features=pd.DataFrame(features.detach().numpy(),index=np.arange(0,features.shape[0]))\n",
    "        Group=pd.Series(init_y,index=np.arange(0,features.shape[0]),name=\"Group\")\n",
    "        Mergefeature=pd.concat([features,Group],axis=1)\n",
    "        cluster_centers=np.asarray(Mergefeature.groupby(\"Group\").mean())\n",
    "        self.mu.data.copy_(torch.Tensor(cluster_centers))\n",
    "        self.train()\n",
    "        for epoch in range(max_epochs):\n",
    "            if epoch%update_interval == 0:\n",
    "                _, q = self.forward(torch.FloatTensor(X),torch.FloatTensor(adj))\n",
    "                p = self.target_distribution(q).data\n",
    "            X=torch.FloatTensor(X)\n",
    "            adj=torch.FloatTensor(adj)\n",
    "            optimizer.zero_grad()\n",
    "            z,q = self(X, adj)\n",
    "            loss = self.loss_function(p, q)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def predict(self, X, adj):\n",
    "        z,q = self(torch.FloatTensor(X),torch.FloatTensor(adj))\n",
    "        return z, q\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2deabd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# util.py\n",
    "\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "from anndata import AnnData,read_csv,read_text,read_mtx\n",
    "from scipy.sparse import issparse\n",
    "import random\n",
    "import torch\n",
    "# from . SpaGCN import SpaGCN\n",
    "# from . calculate_adj import calculate_adj_matrix\n",
    "def prefilter_cells(adata,min_counts=None,max_counts=None,min_genes=200,max_genes=None):\n",
    "    if min_genes is None and min_counts is None and max_genes is None and max_counts is None:\n",
    "        raise ValueError('Provide one of min_counts, min_genes, max_counts or max_genes.')\n",
    "    id_tmp=np.asarray([True]*adata.shape[0],dtype=bool)\n",
    "    id_tmp=np.logical_and(id_tmp,sc.pp.filter_cells(adata.X,min_genes=min_genes)[0]) if min_genes is not None  else id_tmp\n",
    "    id_tmp=np.logical_and(id_tmp,sc.pp.filter_cells(adata.X,max_genes=max_genes)[0]) if max_genes is not None  else id_tmp\n",
    "    id_tmp=np.logical_and(id_tmp,sc.pp.filter_cells(adata.X,min_counts=min_counts)[0]) if min_counts is not None  else id_tmp\n",
    "    id_tmp=np.logical_and(id_tmp,sc.pp.filter_cells(adata.X,max_counts=max_counts)[0]) if max_counts is not None  else id_tmp\n",
    "    adata._inplace_subset_obs(id_tmp)\n",
    "    adata.raw=sc.pp.log1p(adata,copy=True) #check the rowname \n",
    "    print(\"the var_names of adata.raw: adata.raw.var_names.is_unique=:\",adata.raw.var_names.is_unique)\n",
    "    \n",
    "    \n",
    "def prefilter_genes(adata,min_counts=None,max_counts=None,min_cells=10,max_cells=None):\n",
    "    if min_cells is None and min_counts is None and max_cells is None and max_counts is None:\n",
    "        raise ValueError('Provide one of min_counts, min_genes, max_counts or max_genes.')\n",
    "    id_tmp=np.asarray([True]*adata.shape[1],dtype=bool)\n",
    "    id_tmp=np.logical_and(id_tmp,sc.pp.filter_genes(adata.X,min_cells=min_cells)[0]) if min_cells is not None  else id_tmp\n",
    "    id_tmp=np.logical_and(id_tmp,sc.pp.filter_genes(adata.X,max_cells=max_cells)[0]) if max_cells is not None  else id_tmp\n",
    "    id_tmp=np.logical_and(id_tmp,sc.pp.filter_genes(adata.X,min_counts=min_counts)[0]) if min_counts is not None  else id_tmp\n",
    "    id_tmp=np.logical_and(id_tmp,sc.pp.filter_genes(adata.X,max_counts=max_counts)[0]) if max_counts is not None  else id_tmp\n",
    "    adata._inplace_subset_var(id_tmp)\n",
    "\n",
    "\n",
    "def prefilter_specialgenes(adata,Gene1Pattern=\"ERCC\",Gene2Pattern=\"MT-\"):\n",
    "    id_tmp1=np.asarray([not str(name).startswith(Gene1Pattern) for name in adata.var_names],dtype=bool)\n",
    "    id_tmp2=np.asarray([not str(name).startswith(Gene2Pattern) for name in adata.var_names],dtype=bool)\n",
    "    id_tmp=np.logical_and(id_tmp1,id_tmp2)\n",
    "    adata._inplace_subset_var(id_tmp)\n",
    "\n",
    "\n",
    "def calculate_p(adj, l):\n",
    "    adj_exp=np.exp(-1*(adj**2)/(2*(l**2)))\n",
    "    return np.mean(np.sum(adj_exp,1))-1\n",
    "\n",
    "def test_l(adj, list_l):\n",
    "    for l in list_l:\n",
    "        print(\"l is \",str(l),\"Percentage of total expression contributed by neighborhoods:\",calculate_p(adj, l))\n",
    "\n",
    "def find_l(p, adj, start=0.5, end=2,sep=0.01, tol=0.01):\n",
    "    for l in np.arange(start, end, sep):\n",
    "        q=calculate_p(adj, l)\n",
    "        print(\"L=\", str(l), \"P=\", str(round(q,5)))\n",
    "        if np.abs(p-q)<=tol:\n",
    "            return l\n",
    "    print(\"l not found, try bigger range or smaller sep!\")\n",
    "\n",
    "def search_l(p, adj, start=0.01, end=1000, tol=0.01, max_run=100):\n",
    "    run=0\n",
    "    p_low=calculate_p(adj, start)\n",
    "    p_high=calculate_p(adj, end)\n",
    "    if p_low>p+tol:\n",
    "        print(\"l not found, try smaller start point.\")\n",
    "        return None\n",
    "    elif p_high<p-tol:\n",
    "        print(\"l not found, try bigger end point.\")\n",
    "        return None\n",
    "    elif  np.abs(p_low-p) <=tol:\n",
    "        print(\"recommended l = \", str(start))\n",
    "        return start\n",
    "    elif  np.abs(p_high-p) <=tol:\n",
    "        print(\"recommended l = \", str(end))\n",
    "        return end\n",
    "    while (p_low+tol)<p<(p_high-tol):\n",
    "        run+=1\n",
    "        print(\"Run \"+str(run)+\": l [\"+str(start)+\", \"+str(end)+\"], p [\"+str(p_low)+\", \"+str(p_high)+\"]\")\n",
    "        if run >max_run:\n",
    "            print(\"Exact l not found, closest values are:\\n\"+\"l=\"+str(start)+\": \"+\"p=\"+str(p_low)+\"\\nl=\"+str(end)+\": \"+\"p=\"+str(p_high))\n",
    "            return None\n",
    "        mid=(start+end)/2\n",
    "        p_mid=calculate_p(adj, mid)\n",
    "        if np.abs(p_mid-p)<=tol:\n",
    "            print(\"recommended l = \", str(mid))\n",
    "            return mid\n",
    "        if p_mid<=p:\n",
    "            start=mid\n",
    "            p_low=p_mid\n",
    "        else:\n",
    "            end=mid\n",
    "            p_high=p_mid\n",
    "\n",
    "def count_nbr(target_cluster,cell_id, x, y, pred, radius):\n",
    "    adj_2d=calculate_adj_matrix(x=x,y=y, histology=False)\n",
    "    cluster_num = dict()\n",
    "    df = {'cell_id': cell_id, 'x': x, \"y\":y, \"pred\":pred}\n",
    "    df = pd.DataFrame(data=df)\n",
    "    df.index=df['cell_id']\n",
    "    target_df=df[df[\"pred\"]==target_cluster]\n",
    "    row_index=0\n",
    "    num_nbr=[]\n",
    "    for index, row in target_df.iterrows():\n",
    "        x=row[\"x\"]\n",
    "        y=row[\"y\"]\n",
    "        tmp_nbr=df[((df[\"x\"]-x)**2+(df[\"y\"]-y)**2)<=(radius**2)]\n",
    "        num_nbr.append(tmp_nbr.shape[0])\n",
    "    return np.mean(num_nbr)\n",
    "\n",
    "def search_radius(target_cluster,cell_id, x, y, pred, start, end, num_min=8, num_max=15,  max_run=100):\n",
    "    run=0\n",
    "    num_low=count_nbr(target_cluster,cell_id, x, y, pred, start)\n",
    "    num_high=count_nbr(target_cluster,cell_id, x, y, pred, end)\n",
    "    if num_min<=num_low<=num_max:\n",
    "        print(\"recommended radius = \", str(start))\n",
    "        return start\n",
    "    elif num_min<=num_high<=num_max:\n",
    "        print(\"recommended radius = \", str(end))\n",
    "        return end\n",
    "    elif num_low>num_max:\n",
    "        print(\"Try smaller start.\")\n",
    "        return None\n",
    "    elif num_high<num_min:\n",
    "        print(\"Try bigger end.\")\n",
    "        return None\n",
    "    while (num_low<num_min) and (num_high>num_min):\n",
    "        run+=1\n",
    "        print(\"Run \"+str(run)+\": radius [\"+str(start)+\", \"+str(end)+\"], num_nbr [\"+str(num_low)+\", \"+str(num_high)+\"]\")\n",
    "        if run >max_run:\n",
    "            print(\"Exact radius not found, closest values are:\\n\"+\"radius=\"+str(start)+\": \"+\"num_nbr=\"+str(num_low)+\"\\nradius=\"+str(end)+\": \"+\"num_nbr=\"+str(num_high))\n",
    "            return None\n",
    "        mid=(start+end)/2\n",
    "        num_mid=count_nbr(target_cluster,cell_id, x, y, pred, mid)\n",
    "        if num_min<=num_mid<=num_max:\n",
    "            print(\"recommended radius = \", str(mid), \"num_nbr=\"+str(num_mid))\n",
    "            return mid\n",
    "        if num_mid<num_min:\n",
    "            start=mid\n",
    "            num_low=num_mid\n",
    "        elif num_mid>num_max:\n",
    "            end=mid\n",
    "            num_high=num_mid\n",
    "\n",
    "def find_neighbor_clusters(target_cluster,cell_id, x, y, pred,radius, ratio=1/2):\n",
    "    cluster_num = dict()\n",
    "    for i in pred:\n",
    "        cluster_num[i] = cluster_num.get(i, 0) + 1\n",
    "    df = {'cell_id': cell_id, 'x': x, \"y\":y, \"pred\":pred}\n",
    "    df = pd.DataFrame(data=df)\n",
    "    df.index=df['cell_id']\n",
    "    target_df=df[df[\"pred\"]==target_cluster]\n",
    "    nbr_num={}\n",
    "    row_index=0\n",
    "    num_nbr=[]\n",
    "    for index, row in target_df.iterrows():\n",
    "        x=row[\"x\"]\n",
    "        y=row[\"y\"]\n",
    "        tmp_nbr=df[((df[\"x\"]-x)**2+(df[\"y\"]-y)**2)<=(radius**2)]\n",
    "        #tmp_nbr=df[(df[\"x\"]<x+radius) & (df[\"x\"]>x-radius) & (df[\"y\"]<y+radius) & (df[\"y\"]>y-radius)]\n",
    "        num_nbr.append(tmp_nbr.shape[0])\n",
    "        for p in tmp_nbr[\"pred\"]:\n",
    "            nbr_num[p]=nbr_num.get(p,0)+1\n",
    "    del nbr_num[target_cluster]\n",
    "    nbr_num=[(k, v)  for k, v in nbr_num.items() if v>(ratio*cluster_num[k])]\n",
    "    nbr_num.sort(key=lambda x: -x[1])\n",
    "    print(\"radius=\", radius, \"average number of neighbors for each spot is\", np.mean(num_nbr))\n",
    "    print(\" Cluster\",target_cluster, \"has neighbors:\")\n",
    "    for t in nbr_num:\n",
    "        print(\"Dmain \", t[0], \": \",t[1])\n",
    "    ret=[t[0] for t in nbr_num]\n",
    "    if len(ret)==0:\n",
    "        print(\"No neighbor domain found, try bigger radius or smaller ratio.\")\n",
    "    else:\n",
    "        return ret\n",
    "\n",
    "\n",
    "def rank_genes_groups(input_adata, target_cluster,nbr_list, label_col, adj_nbr=True, log=False):\n",
    "    if adj_nbr:\n",
    "        nbr_list=nbr_list+[target_cluster]\n",
    "        adata=input_adata[input_adata.obs[label_col].isin(nbr_list)]\n",
    "    else:\n",
    "        adata=input_adata.copy()\n",
    "    adata.var_names_make_unique()\n",
    "    adata.obs[\"target\"]=((adata.obs[label_col]==target_cluster)*1).astype('category')\n",
    "    sc.tl.rank_genes_groups(adata, groupby=\"target\",reference=\"rest\", n_genes=adata.shape[1],method='wilcoxon')\n",
    "    pvals_adj=[i[1] for i in adata.uns['rank_genes_groups'][\"pvals_adj\"]]\n",
    "    genes=[i[1] for i in adata.uns['rank_genes_groups'][\"names\"]]\n",
    "    if issparse(adata.X):\n",
    "        obs_tidy=pd.DataFrame(adata.X.A)\n",
    "    else:\n",
    "        obs_tidy=pd.DataFrame(adata.X)\n",
    "    obs_tidy.index=adata.obs[\"target\"].tolist()\n",
    "    obs_tidy.columns=adata.var.index.tolist()\n",
    "    obs_tidy=obs_tidy.loc[:,genes]\n",
    "    # 1. compute mean value\n",
    "    mean_obs = obs_tidy.groupby(level=0).mean()\n",
    "    # 2. compute fraction of cells having value >0\n",
    "    obs_bool = obs_tidy.astype(bool)\n",
    "    fraction_obs = obs_bool.groupby(level=0).sum() / obs_bool.groupby(level=0).count()\n",
    "    # compute fold change.\n",
    "    if log: #The adata already logged\n",
    "        fold_change=np.exp((mean_obs.loc[1] - mean_obs.loc[0]).values)\n",
    "    else:\n",
    "        fold_change = (mean_obs.loc[1] / (mean_obs.loc[0]+ 1e-9)).values\n",
    "    df = {'genes': genes, 'in_group_fraction': fraction_obs.loc[1].tolist(), \"out_group_fraction\":fraction_obs.loc[0].tolist(),\"in_out_group_ratio\":(fraction_obs.loc[1]/fraction_obs.loc[0]).tolist(),\"in_group_mean_exp\": mean_obs.loc[1].tolist(), \"out_group_mean_exp\": mean_obs.loc[0].tolist(),\"fold_change\":fold_change.tolist(), \"pvals_adj\":pvals_adj}\n",
    "    df = pd.DataFrame(data=df)\n",
    "    return df\n",
    "\n",
    "def relative_func(expres):\n",
    "    #expres: an array counts expression for a gene\n",
    "    maxd = np.max(expres) - np.min(expres)\n",
    "    min_exp=np.min(expres)\n",
    "    rexpr = (expres - min_exp)/maxd\n",
    "    return rexpr\n",
    "\n",
    "def plot_relative_exp(input_adata, gene, x_name, y_name,color,use_raw=False, spot_size=200000):\n",
    "    adata=input_adata.copy()\n",
    "    if use_raw:\n",
    "        X=adata.raw.X\n",
    "    else:\n",
    "        X=adata.X\n",
    "    if issparse(X):\n",
    "        X=pd.DataFrame(X.A)\n",
    "    else:\n",
    "        X=pd.DataFrame(X)\n",
    "    X.index=adata.obs.index\n",
    "    X.columns=adata.var.index\n",
    "    rexpr=relative_func(X.loc[:,gene])\n",
    "    adata.obs[\"rexpr\"]=rexpr\n",
    "    fig=sc.pl.scatter(adata,x=x_name,y=y_name,color=\"rexpr\",title=gene+\"_rexpr\",color_map=color,show=False,size=spot_size/adata.shape[0])\n",
    "    return fig\n",
    "\n",
    "def plot_log_exp(input_adata, gene, x_name, y_name,color,use_raw=False):\n",
    "    adata=input_adata.copy()\n",
    "    if use_raw:\n",
    "        X=adata.X\n",
    "    else:\n",
    "        X=adata.raw.X\n",
    "    if issparse(X):\n",
    "        X=pd.DataFrame(X.A)\n",
    "    else:\n",
    "        X=pd.DataFrame(X)\n",
    "    X.index=adata.obs.index\n",
    "    X.columns=adata.var.index\n",
    "    adata.obs[\"log\"]=np.log((X.loc[:,gene]+1).tolist())\n",
    "    fig=sc.pl.scatter(adata,x=x_name,y=y_name,color=\"log\",title=gene+\"_log\",color_map=color,show=False,size=200000/adata.shape[0])\n",
    "    return fig\n",
    "\n",
    "def detect_subclusters(cell_id, x, y, pred, target_cluster, radius=3, res=0.2):\n",
    "    df = {'cell_id': cell_id, 'x': x, \"y\":y, \"pred\":pred}\n",
    "    df = pd.DataFrame(data=df)\n",
    "    df.index=df['cell_id']\n",
    "    target_df=df[df[\"pred\"]==target_cluster]\n",
    "    nbr=np.zeros([target_df.shape[0],len(set(df[\"pred\"]))],dtype=int)\n",
    "    num_nbr=[]\n",
    "    row_index=0\n",
    "    for index, row in target_df.iterrows():\n",
    "        x=row[\"x\"]\n",
    "        y=row[\"y\"]\n",
    "        tmp_nbr=df[(df[\"x\"]<x+radius) & (df[\"x\"]>x-radius) & (df[\"y\"]<y+radius) & (df[\"y\"]>y-radius)]\n",
    "        num_nbr.append(tmp_nbr.shape[0])\n",
    "        for p in tmp_nbr[\"pred\"]:\n",
    "            nbr[row_index,int(p)]+=1\n",
    "        row_index+=1\n",
    "    #Minus out the cell itself\n",
    "    nbr[:,target_cluster]=nbr[:,target_cluster]-1\n",
    "    nbr=sc.AnnData(nbr)\n",
    "    sc.pp.neighbors(nbr, n_neighbors=10)\n",
    "    sc.tl.louvain(nbr,resolution=res)\n",
    "    sub_cluster=nbr.obs['louvain'].astype(int).to_numpy()\n",
    "    target_df[\"sub_cluster\"]=sub_cluster\n",
    "    target_df[\"sub_cluster\"]=target_df[\"sub_cluster\"].astype('category')\n",
    "    tmp=[]\n",
    "    for j in df.index:\n",
    "        if j in target_df.index:\n",
    "            tmp.append(target_df.loc[j,\"sub_cluster\"])\n",
    "        else:\n",
    "            tmp.append(\"-1\")\n",
    "    #ret = {'cell_id': cell_id, 'sub_cluster_'+str(target_cluster): tmp}\n",
    "    #ret = pd.DataFrame(data=ret)\n",
    "    #ret.index=ret['cell_id']\n",
    "    ret=tmp\n",
    "    return ret\n",
    "\n",
    "def find_meta_gene(input_adata,\n",
    "                    pred,\n",
    "                    target_domain,\n",
    "                    start_gene,\n",
    "                    mean_diff=0,\n",
    "                    early_stop=True,\n",
    "                    max_iter=5,\n",
    "                    use_raw=False):\n",
    "    meta_name=start_gene\n",
    "    adata=input_adata.copy()\n",
    "    adata.obs[\"meta\"]=adata.X[:,adata.var.index==start_gene]\n",
    "    adata.obs[\"pred\"]=pred\n",
    "    num_non_target=adata.shape[0]\n",
    "    for i in range(max_iter):\n",
    "        #Select cells\n",
    "        tmp=adata[((adata.obs[\"meta\"]>np.mean(adata.obs[adata.obs[\"pred\"]==target_domain][\"meta\"]))|(adata.obs[\"pred\"]==target_domain))]\n",
    "        tmp.obs[\"target\"]=((tmp.obs[\"pred\"]==target_domain)*1).astype('category').copy()\n",
    "        if (len(set(tmp.obs[\"target\"]))<2) or (np.min(tmp.obs[\"target\"].value_counts().values)<5):\n",
    "            print(\"Meta gene is: \", meta_name)\n",
    "            return meta_name, adata.obs[\"meta\"].tolist()\n",
    "        #DE\n",
    "        sc.tl.rank_genes_groups(tmp, groupby=\"target\",reference=\"rest\", n_genes=1,method='wilcoxon')\n",
    "        adj_g=tmp.uns['rank_genes_groups'][\"names\"][0][0]\n",
    "        add_g=tmp.uns['rank_genes_groups'][\"names\"][0][1]\n",
    "        meta_name_cur=meta_name+\"+\"+add_g+\"-\"+adj_g\n",
    "        print(\"Add gene: \", add_g)\n",
    "        print(\"Minus gene: \", adj_g)\n",
    "        #Meta gene\n",
    "        adata.obs[add_g]=adata.X[:,adata.var.index==add_g]\n",
    "        adata.obs[adj_g]=adata.X[:,adata.var.index==adj_g]\n",
    "        adata.obs[\"meta_cur\"]=(adata.obs[\"meta\"]+adata.obs[add_g]-adata.obs[adj_g])\n",
    "        adata.obs[\"meta_cur\"]=adata.obs[\"meta_cur\"]-np.min(adata.obs[\"meta_cur\"])\n",
    "        mean_diff_cur=np.mean(adata.obs[\"meta_cur\"][adata.obs[\"pred\"]==target_domain])-np.mean(adata.obs[\"meta_cur\"][adata.obs[\"pred\"]!=target_domain])\n",
    "        num_non_target_cur=np.sum(tmp.obs[\"target\"]==0)\n",
    "        if (early_stop==False) | ((num_non_target>=num_non_target_cur) & (mean_diff<=mean_diff_cur)):\n",
    "            num_non_target=num_non_target_cur\n",
    "            mean_diff=mean_diff_cur\n",
    "            print(\"Absolute mean change:\", mean_diff)\n",
    "            print(\"Number of non-target spots reduced to:\",num_non_target)\n",
    "        else:\n",
    "            print(\"Stopped!\", \"Previous Number of non-target spots\",num_non_target, num_non_target_cur, mean_diff,mean_diff_cur)\n",
    "            print(\"Previous Number of non-target spots\",num_non_target, num_non_target_cur, mean_diff,mean_diff_cur)\n",
    "            print(\"Previous Number of non-target spots\",num_non_target)\n",
    "            print(\"Current Number of non-target spots\",num_non_target_cur)\n",
    "            print(\"Absolute mean change\", mean_diff)\n",
    "            print(\"===========================================================================\")\n",
    "            print(\"Meta gene: \", meta_name)\n",
    "            print(\"===========================================================================\")\n",
    "            return meta_name, adata.obs[\"meta\"].tolist()\n",
    "        meta_name=meta_name_cur\n",
    "        adata.obs[\"meta\"]=adata.obs[\"meta_cur\"]\n",
    "        print(\"===========================================================================\")\n",
    "        print(\"Meta gene is: \", meta_name)\n",
    "        print(\"===========================================================================\")\n",
    "    return meta_name, adata.obs[\"meta\"].tolist()\n",
    "\n",
    "\n",
    "def search_res(adata, adj, l, target_num, start=0.4, step=0.1, tol=5e-3, lr=0.05, max_epochs=10, r_seed=100, t_seed=100, n_seed=100, max_run=10):\n",
    "    random.seed(r_seed)\n",
    "    torch.manual_seed(t_seed)\n",
    "    np.random.seed(n_seed)\n",
    "    res=start\n",
    "    print(\"Start at res = \", res, \"step = \", step)\n",
    "    clf=SpaGCN()\n",
    "    clf.set_l(l)\n",
    "    clf.train(adata,adj,init_spa=True,init=\"louvain\",res=res, tol=tol, lr=lr, max_epochs=max_epochs)\n",
    "    y_pred, _=clf.predict()\n",
    "    old_num=len(set(y_pred))\n",
    "    print(\"Res = \", res, \"Num of clusters = \", old_num)\n",
    "    run=0\n",
    "    while old_num!=target_num:\n",
    "        random.seed(r_seed)\n",
    "        torch.manual_seed(t_seed)\n",
    "        np.random.seed(n_seed)\n",
    "        old_sign=1 if (old_num<target_num) else -1\n",
    "        clf=SpaGCN()\n",
    "        clf.set_l(l)\n",
    "        clf.train(adata,adj,init_spa=True,init=\"louvain\",res=res+step*old_sign, tol=tol, lr=lr, max_epochs=max_epochs)\n",
    "        y_pred, _=clf.predict()\n",
    "        new_num=len(set(y_pred))\n",
    "        print(\"Res = \", res+step*old_sign, \"Num of clusters = \", new_num)\n",
    "        if new_num==target_num:\n",
    "            res=res+step*old_sign\n",
    "            print(\"recommended res = \", str(res))\n",
    "            return res\n",
    "        new_sign=1 if (new_num<target_num) else -1\n",
    "        if new_sign==old_sign:\n",
    "            res=res+step*old_sign\n",
    "            print(\"Res changed to\", res)\n",
    "            old_num=new_num\n",
    "        else:\n",
    "            step=step/2\n",
    "            print(\"Step changed to\", step)\n",
    "        if run >max_run:\n",
    "            print(\"Exact resolution not found\")\n",
    "            print(\"Recommended res = \", str(res))\n",
    "            return res\n",
    "        run+=1\n",
    "    print(\"recommended res = \", str(res))\n",
    "    return res\n",
    "\n",
    "\n",
    "def refine(sample_id, pred, dis, shape=\"hexagon\"):\n",
    "    refined_pred=[]\n",
    "    pred=pd.DataFrame({\"pred\": pred}, index=sample_id)\n",
    "    dis_df=pd.DataFrame(dis, index=sample_id, columns=sample_id)\n",
    "    if shape==\"hexagon\":\n",
    "        num_nbs=6 \n",
    "    elif shape==\"square\":\n",
    "        num_nbs=4\n",
    "    else:\n",
    "        print(\"Shape not recongized, shape='hexagon' for Visium data, 'square' for ST data.\")\n",
    "    for i in range(len(sample_id)):\n",
    "        index=sample_id[i]\n",
    "        dis_tmp=dis_df.loc[index, :].sort_values()\n",
    "        nbs=dis_tmp[0:num_nbs+1]\n",
    "        nbs_pred=pred.loc[nbs.index, \"pred\"]\n",
    "        self_pred=pred.loc[index, \"pred\"]\n",
    "        v_c=nbs_pred.value_counts()\n",
    "        if (v_c.loc[self_pred]<num_nbs/2) and (np.max(v_c)>num_nbs/2):\n",
    "            refined_pred.append(v_c.idxmax())\n",
    "        else:           \n",
    "            refined_pred.append(self_pred)\n",
    "    return refined_pred\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da0279f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpaGCN.py\n",
    "\n",
    "import os,csv,re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from scipy.sparse import issparse\n",
    "from anndata import AnnData\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "import matplotlib.colors as clr\n",
    "import matplotlib.pyplot as plt\n",
    "# from . models import *\n",
    "\n",
    "\n",
    "class SpaGCN(object):\n",
    "    def __init__(self):\n",
    "        super(SpaGCN, self).__init__()\n",
    "        self.l=None\n",
    "\n",
    "    def set_l(self, l):\n",
    "        self.l=l\n",
    "\n",
    "    def train(self,adata,adj, \n",
    "            num_pcs=50, \n",
    "            lr=0.005,\n",
    "            max_epochs=2000,\n",
    "            weight_decay=0,\n",
    "            opt=\"admin\",\n",
    "            init_spa=True,\n",
    "            init=\"louvain\", #louvain or kmeans\n",
    "            n_neighbors=10, #for louvain\n",
    "            n_clusters=None, #for kmeans\n",
    "            res=0.4, #for louvain\n",
    "            tol=1e-3):\n",
    "        self.num_pcs=num_pcs\n",
    "        self.res=res\n",
    "        self.lr=lr\n",
    "        self.max_epochs=max_epochs\n",
    "        self.weight_decay=weight_decay\n",
    "        self.opt=opt\n",
    "        self.init_spa=init_spa\n",
    "        self.init=init\n",
    "        self.n_neighbors=n_neighbors\n",
    "        self.n_clusters=n_clusters\n",
    "        self.res=res\n",
    "        self.tol=tol\n",
    "        assert adata.shape[0]==adj.shape[0]==adj.shape[1]\n",
    "        pca = PCA(n_components=self.num_pcs)\n",
    "        if issparse(adata.X):\n",
    "            pca.fit(adata.X.A)\n",
    "            embed=pca.transform(adata.X.A)\n",
    "        else:\n",
    "            pca.fit(adata.X)\n",
    "            embed=pca.transform(adata.X)\n",
    "        ###------------------------------------------###\n",
    "        if self.l is None:\n",
    "            raise ValueError('l should be set before fitting the model!')\n",
    "        adj_exp=np.exp(-1*(adj**2)/(2*(self.l**2)))\n",
    "        #----------Train model----------\n",
    "        self.model=simple_GC_DEC(embed.shape[1],embed.shape[1])\n",
    "        self.model.fit(embed,adj_exp,lr=self.lr,max_epochs=self.max_epochs,weight_decay=self.weight_decay,opt=self.opt,init_spa=self.init_spa,init=self.init,n_neighbors=self.n_neighbors,n_clusters=self.n_clusters,res=self.res, tol=self.tol)\n",
    "        self.embed=embed\n",
    "        self.adj_exp=adj_exp\n",
    "\n",
    "    def predict(self):\n",
    "        z,q=self.model.predict(self.embed,self.adj_exp)\n",
    "        y_pred = torch.argmax(q, dim=1).data.cpu().numpy()\n",
    "        # Max probability plot\n",
    "        prob=q.detach().numpy()\n",
    "        return y_pred, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69ef47b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_spatial_domains_ez_mode(adata, img, x_array, y_array, x_pixel, y_pixel, n_clusters, histology=True, s=1, b=49, p=0.5, r_seed=100, t_seed=100, n_seed=100):\n",
    "    adj=calculate_adj_matrix(x=x_pixel,y=y_pixel, x_pixel=x_pixel, y_pixel=y_pixel, image=img, beta=b, alpha=s, histology=histology)\n",
    "#     adj=calculate_adj_matrix(g=img_g, alpha=s, histology=histology)\n",
    "    prefilter_genes(adata,min_cells=3) # avoiding all genes are zeros\n",
    "    prefilter_specialgenes(adata)\n",
    "    sc.pp.normalize_per_cell(adata)\n",
    "    sc.pp.log1p(adata)\n",
    "    l=search_l(p, adj, start=0.01, end=1000, tol=0.01, max_run=100)\n",
    "    res=search_res(adata, adj, l, n_clusters, start=0.7, step=0.1, tol=5e-3, lr=0.05, max_epochs=20, r_seed=r_seed, t_seed=t_seed, n_seed=n_seed)\n",
    "    clf=SpaGCN()\n",
    "    clf.set_l(l)\n",
    "    random.seed(r_seed)\n",
    "    torch.manual_seed(t_seed)\n",
    "    np.random.seed(n_seed)\n",
    "    clf.train(adata,adj,init_spa=True,init=\"louvain\",res=res, tol=5e-3, lr=0.05, max_epochs=200)\n",
    "    y_pred, prob=clf.predict()\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def calculate_adj_matrix(x, y, x_pixel=None, y_pixel=None, image=None, beta=49, alpha=1, histology=True):\n",
    "\t#x,y,x_pixel, y_pixel are lists\n",
    "\tif histology:\n",
    "\t\tassert (x_pixel is not None) & (x_pixel is not None) & (image is not None)\n",
    "\t\tassert (len(x)==len(x_pixel)) & (len(y)==len(y_pixel))\n",
    "\t\tprint(\"Calculateing adj matrix using histology image...\")\n",
    "\t\t#beta to control the range of neighbourhood when calculate grey vale for one spot\n",
    "\t\t#alpha to control the color scale\n",
    "\t\tbeta_half=round(beta/2)\n",
    "\t\tg=[]\n",
    "\t\tfor i in range(len(x_pixel)):\n",
    "\t\t\tmax_x=image.shape[0]\n",
    "\t\t\tmax_y=image.shape[1]\n",
    "\t\t\tnbs=image[max(0,x_pixel[i]-beta_half):min(max_x,x_pixel[i]+beta_half+1),max(0,y_pixel[i]-beta_half):min(max_y,y_pixel[i]+beta_half+1)]\n",
    "\t\t\tg.append(np.mean(np.mean(nbs,axis=0),axis=0))\n",
    "\t\tc0, c1, c2=[], [], []\n",
    "\t\tfor i in g:\n",
    "\t\t\tc0.append(i[0])\n",
    "\t\t\tc1.append(i[1])\n",
    "\t\t\tc2.append(i[2])\n",
    "\t\tc0=np.array(c0)\n",
    "\t\tc1=np.array(c1)\n",
    "\t\tc2=np.array(c2)\n",
    "\t\tprint(\"Var of c0,c1,c2 = \", np.var(c0),np.var(c1),np.var(c2))\n",
    "\t\tc3=(c0*np.var(c0)+c1*np.var(c1)+c2*np.var(c2))/(np.var(c0)+np.var(c1)+np.var(c2))\n",
    "\t\tc4=(c3-np.mean(c3))/np.std(c3)\n",
    "\t\tz_scale=np.max([np.std(x), np.std(y)])*alpha\n",
    "\t\tz=c4*z_scale\n",
    "\t\tz=z.tolist()\n",
    "\t\tprint(\"Var of x,y,z = \", np.var(x),np.var(y),np.var(z))\n",
    "\t\tX=np.array([x, y, z]).T.astype(np.float32)\n",
    "\telse:\n",
    "\t\tprint(\"Calculateing adj matrix using xy only...\")\n",
    "\t\tX=np.array([x, y]).T.astype(np.float32)\n",
    "\treturn pairwise_distance(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb32d1d2",
   "metadata": {},
   "source": [
    "## load adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11442021",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata=sc.read(\"./data/151673/sample_data.h5ad\")\n",
    "#Read in hitology image\n",
    "img=cv2.imread(\"./data/151673/histology.tif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_38",
   "language": "python",
   "name": "tf2_38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
